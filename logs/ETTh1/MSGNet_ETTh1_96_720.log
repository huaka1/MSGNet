Args in experiment:
Namespace(activation='gelu', batch_size=32, c_out=7, checkpoints='./checkpoints/', conv_channel=32, d_ff=32, d_layers=1, d_model=16, data='ETTh1', data_path='ETTh1.csv', dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.1, e_layers=1, embed='timeF', embed_type=0, enc_in=7, factor=3, features='M', freq='h', gcn_depth=2, gcn_dropout=0.3, gpu=0, individual=False, is_training=1, itr=1, label_len=48, learning_rate=0.0001, loss='MSE', lradj='type1', model='MSGNet', model_id='ETTh1_96_720', moving_avg=25, n_heads=8, node_dim=10, num_kernels=6, num_nodes=7, num_workers=8, output_attention=False, patience=3, pred_len=720, propalpha=0.3, root_path='./dataset/', seasonal_patterns='Monthly', seq_len=96, skip_channel=32, subgraph_size=3, tanhalpha=3, target='OT', task_name='long_term_forecast', test_flop=False, top_k=3, train_epochs=10, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_96_720_MSGNet_ETTh1_ftM_sl96_ll48_pl720_dm16_nh8_el1_dl1_df32_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2161
test 2161
	iters: 100, epoch: 1 | loss: 0.6949247
	speed: 0.0556s/iter; left time: 130.0623s
	iters: 200, epoch: 1 | loss: 0.7116194
	speed: 0.0407s/iter; left time: 91.1746s
Epoch: 1 cost time: 11.609038591384888
Epoch: 1, Steps: 244 | Train Loss: 0.7255433 Vali Loss: 1.6546589 Test Loss: 0.5348536
Validation loss decreased (inf --> 1.654659).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6315055
	speed: 0.1177s/iter; left time: 246.8722s
	iters: 200, epoch: 2 | loss: 0.6741799
	speed: 0.0415s/iter; left time: 82.8468s
Epoch: 2 cost time: 11.717068672180176
Epoch: 2, Steps: 244 | Train Loss: 0.6451076 Vali Loss: 1.6093451 Test Loss: 0.4971648
Validation loss decreased (1.654659 --> 1.609345).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.5205864
	speed: 0.1162s/iter; left time: 215.3721s
	iters: 200, epoch: 3 | loss: 0.5586064
	speed: 0.0377s/iter; left time: 66.0565s
Epoch: 3 cost time: 10.82015609741211
Epoch: 3, Steps: 244 | Train Loss: 0.6184089 Vali Loss: 1.5893294 Test Loss: 0.4910885
Validation loss decreased (1.609345 --> 1.589329).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.5572539
	speed: 0.1107s/iter; left time: 178.1123s
	iters: 200, epoch: 4 | loss: 0.5516157
	speed: 0.0379s/iter; left time: 57.1993s
Epoch: 4 cost time: 10.683793306350708
Epoch: 4, Steps: 244 | Train Loss: 0.6010431 Vali Loss: 1.5891294 Test Loss: 0.4953368
Validation loss decreased (1.589329 --> 1.589129).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.7008893
	speed: 0.1104s/iter; left time: 150.7586s
	iters: 200, epoch: 5 | loss: 0.5955260
	speed: 0.0393s/iter; left time: 49.6570s
Epoch: 5 cost time: 10.979138612747192
Epoch: 5, Steps: 244 | Train Loss: 0.5922623 Vali Loss: 1.5918102 Test Loss: 0.4914502
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.5245408
	speed: 0.1116s/iter; left time: 125.1299s
	iters: 200, epoch: 6 | loss: 0.6203302
	speed: 0.0369s/iter; left time: 37.6681s
Epoch: 6 cost time: 10.575998067855835
Epoch: 6, Steps: 244 | Train Loss: 0.5881832 Vali Loss: 1.5919353 Test Loss: 0.4948545
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.5369975
	speed: 0.1098s/iter; left time: 96.2953s
	iters: 200, epoch: 7 | loss: 0.5905108
	speed: 0.0378s/iter; left time: 29.3766s
Epoch: 7 cost time: 10.764400243759155
Epoch: 7, Steps: 244 | Train Loss: 0.5865527 Vali Loss: 1.5917475 Test Loss: 0.4958934
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_96_720_MSGNet_ETTh1_ftM_sl96_ll48_pl720_dm16_nh8_el1_dl1_df32_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
preds_shape: (67, 32, 720, 7)
trues_shape: (67, 32, 720, 7)
nd:0.6108912825584412, nrmse:0.878242552280426, mse:0.4953368306159973, mae:0.4895530641078949, rse:0.6741048693656921, mape:11.29538631439209
time: 118.01911115646362
